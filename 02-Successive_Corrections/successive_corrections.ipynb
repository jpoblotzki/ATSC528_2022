{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74e2cdc4-e6f5-41b2-b35c-3fef1dcffdc9",
   "metadata": {},
   "source": [
    "Assignment: 02-Successive Corrections\n",
    "Author: Julia Poblotzki\n",
    "Due: November 4, 2022 @ 11:59pm\n",
    "\n",
    "\"\"\"\n",
    "Created By    : Jared W. Marquis\n",
    "Creation Date : 01 August 2022\n",
    "Course        : ATSC 528 - Atmospheric Data Analysis\n",
    "Assignment    : 02 - Successive Corrections\n",
    "\n",
    "Purpose:\n",
    "Script to take sparse upper air observations and analyze them on a polar stereographic map projection using successive corrections.\n",
    "\n",
    " >> Function fitting is tough since you need to choose a proper basis function which can lead to overfitting/underfitting/etc. Successive\n",
    "    corrections can remedy this partly by leveraging background and observation values.\n",
    " >> first, define functions for the Cressman weight function & the bilinear interpolation to be used later in the analysis\n",
    " >> next up, is defining base variables, setting up an analysis grid, and plotting obs & analysis locations on one map to \n",
    "    double check everything was done correctly\n",
    " >> next, set up the RoI to be used for each pass by calculating dmin which is the average minimum distance from one ob to another\n",
    "     >> the RoI will decrease with each pass 4*dmin --> 2.5*dmin --> 1.5*dmin\n",
    " >> now it is time for the passes!\n",
    "     >> let's define variables first though: \n",
    "         fa = analysis values @ grid points; \n",
    "         fo = obs values @ obs locaions; \n",
    "         fb = analysis values @ obs locations\n",
    "     >> first pass is easy as there is no background, so it's just: fa = sum(weights * fo) / sum(weights)\n",
    "     >> second pass is trickier since we need to include a background at our obs locations BUT WE DON'T EXACTLY HAVE ONE\n",
    "         >> use bilinear interpolation or Cressman weights (dependent on location within domain or not) to approximate background at obs \n",
    "            locations by using the previous' analysis values\n",
    "         >> once that is done, the pass can be done: fa = fa(prev. run) + sum(weights * (fo-fb)) / sum(weights)\n",
    "     >> third pass is same principle as second pass (woohoo! easy now)\n",
    " >> the passes are done and our analysis matrix is filled... how did they compare?\n",
    "     >> analysis differences: literally just the differences between each pass\n",
    "     >> root mean squared error: rms = sqrt (sum(fo-fb)²/K) where K is total non-nan obs\n",
    "~fin\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335135f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import Required Modules ###\n",
    "import numpy as np                 #numpy for math\n",
    "import pandas as pd                #pandas because they're cute\n",
    "import matplotlib.pyplot as plt    #matplotlib for plotting\n",
    "import cartopy.crs as ccrs         #cartopy for plotting on map\n",
    "import cartopy.feature as cfeature #cartopy basic shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d315ece-a1d0-4f39-9ced-9d66d89a8fee",
   "metadata": {},
   "source": [
    "### CREATE FUNCTIONS TO BE CALLED LATER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc35da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create function for Cressman Analysis ###\n",
    "def cressman(dik, R):\n",
    "    if dik <= R:\n",
    "        w = (R**2 - dik**2) / (R**2 + dik**2)\n",
    "    \n",
    "    elif dik > R:\n",
    "        w = 0\n",
    "        \n",
    "    else:\n",
    "        print(\"Das hat nicht so ganz geklappt wie geplant\")\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac991609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create function for bilinear interpolation ###\n",
    "\n",
    "def bilinear(x, y, ROI):\n",
    "    rel_ind_x = x - np.floor(x)\n",
    "    rel_ind_y = y - np.floor(y)\n",
    "    \n",
    "    z1 = f_A[int(np.floor(y)),int(np.floor(x)),ROI]\n",
    "    z2 = f_A[int(np.ceil(y)),int(np.floor(x)),ROI]\n",
    "    z3 = f_A[int(np.ceil(y)),int(np.ceil(x)),ROI]\n",
    "    z4 = f_A[int(np.floor(y)),int(np.ceil(x)),ROI]\n",
    "    \n",
    "    P = (((rel_ind_x-1)*(rel_ind_y-1)*z1)-((rel_ind_x-0)*(rel_ind_y-1)*z2)+((rel_ind_x-0)*(rel_ind_y-0)*z3)-((rel_ind_x-1)*(rel_ind_y-0)*z4))\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8486681-9ec0-4bc6-96ff-837a4253cfcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DEFINE VARIABLES & SET UP ANALYSIS GRID ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bf048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### define variables ###\n",
    "xo = 18.9\n",
    "yo = -6.3\n",
    "delta_x = 1.27\n",
    "delta_y = 1.27\n",
    "phi_o = 60\n",
    "lambda_o = -115\n",
    "rho = 637100000. #convert from km to cm\n",
    "map_scale = 1/15000000\n",
    "\n",
    "### Read in observations ###\n",
    "file = open(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\RAOBs_201903131200.txt\", \"r\")\n",
    "data= pd.read_csv(file, sep=\",\")\n",
    "data[data.columns[1:5]]=data[data.columns[1:5]].astype(float) ## make text to numbers\n",
    "\n",
    "fo = data['Height'].values #500-mb height values for obs\n",
    "fo = np.array(fo) #do this due to those black & white fluffballs aka pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1e856-d084-41fe-81a6-9eef109c725d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### convert obs lat/long to x,y ###\n",
    "\n",
    "## convert obs from lat,lon to x,y\n",
    "sigma = ((1.0+(np.sin(phi_o * np.pi / 180.))) / (1.0+(np.sin(data['Lat'] * np.pi / 180.))))\n",
    "x_ob = sigma * rho * np.cos(data['Lat'] * np.pi / 180.) * np.cos((data['Lon']-lambda_o) * np.pi / 180.) * map_scale\n",
    "y_ob = sigma * rho * np.cos(data['Lat'] * np.pi / 180.) * np.sin((data['Lon']-lambda_o) * np.pi / 180.) * map_scale\n",
    "\n",
    "# make our obs x & y into an array for easier math-ing\n",
    "x_ob = np.array(x_ob)\n",
    "y_ob = np.array(y_ob)\n",
    "\n",
    "## convert obs x,y back to lat,lon to plot\n",
    "data_lon = np.arctan(y_ob/x_ob)*(180/np.pi)+lambda_o\n",
    "data_lat=(180/np.pi)*((np.pi/2)-(2*np.arctan(np.sqrt((x_ob/map_scale)**2+(y_ob/map_scale)**2)/(rho*(1+np.sin(phi_o*np.pi/180.))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464917a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Set up analysis map with a 22x28 rectangular grid of points ###\n",
    "\n",
    "## make a grid\n",
    "xg = xo+np.arange(22)*delta_x\n",
    "yg = yo+np.arange(28)*delta_y\n",
    "x_grid, y_grid = np.meshgrid(xg,yg)\n",
    "\n",
    "## convert grid from x,y to lat,lon for plotting purposes\n",
    "grid_lon = np.arctan(y_grid/x_grid)*(180/np.pi)+lambda_o\n",
    "grid_lat=(180/np.pi)*((np.pi/2)-(2*np.arctan(np.sqrt((x_grid/map_scale)**2+(y_grid/map_scale)**2)/(rho*(1+np.sin(phi_o*np.pi/180.))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc880f4f-f670-4244-8a03-467da1974c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## plot obs and grid locations for verification ##\n",
    "\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "ax1.scatter(grid_lon.ravel(), grid_lat.ravel(), c='silver', marker='.', transform = ccrs.PlateCarree())\n",
    "ax1.scatter(data_lon, data_lat, c='rosybrown', marker='*', transform = ccrs.PlateCarree())\n",
    "\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Obs_and_Grid_Locations.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53b5ca-12f4-4ad2-86ee-c05612119c8e",
   "metadata": {},
   "source": [
    "### PERFORM 500mb GEOPOTENTIAL HEIGHT ANALYSES USING A CRESSMAN WEIGHTING FUNCTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5634ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### SET UP RADII OF INFLUENCES ###\n",
    "\n",
    "## calculate d_min since using radii of influence of 4, 2.5, & 1.5 *dmin\n",
    "#d_min = closest ob to your ob on avg..... one number for all! (= 2.5548)\n",
    "\n",
    "dist = np.zeros(len(x_ob))\n",
    "for ü in range(len(x_ob)):\n",
    "    d = (((x_ob[ü]-x_ob)**2)+((y_ob[ü]-y_ob)**2))**(1/2) #get the distance\n",
    "    d = np.delete(d, ü) # delete the ü've value as this value would be 0 (distance from itself)\n",
    "    dist[ü]=np.min(d) # get the minimum distance for each ob & store in an array\n",
    "\n",
    "#now get the avg minimum distance\n",
    "dmin = np.mean(dist)\n",
    "\n",
    "## define the ROIs\n",
    "R1 = 4 * dmin     # first pass' RoI\n",
    "R2 = 2.5 * dmin   # second pass' RoI\n",
    "R3 = 1.5 * dmin   # third pass' RoI\n",
    "\n",
    "# define radius of influence matrix\n",
    "ROI_arr = np.array([R1,R2,R3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4450fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### FIRST ANALYSIS; NO BACKGROUND ###\n",
    "## creating analysis values at grid points for proxy background values at grid points... \n",
    "# ...in order to then create 'background values' at obs locations in subsequent passes\n",
    "\n",
    "## fa = sum(weights * fo) / sum(weights)\n",
    "\n",
    "## empty matrix to be filled later with the analysis values\n",
    "y_n=22\n",
    "x_n=28\n",
    "f_A = np.zeros((x_n, y_n, len(ROI_arr))) ##for our anaysis values\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "# first pass (no background): fa = sum(weights * fo) / sum(weights) --> (f_A (r_i))\n",
    "for m in range(len(x_grid)):\n",
    "    for n in range(len(x_grid[0])):\n",
    "        rel_x = x_grid[m,n] - x_ob ## subtract your obs from your analysis to get...\n",
    "        rel_y = y_grid[m,n] - y_ob ## ...relative distance for our map\n",
    "        radius = (rel_x**2 + rel_y**2)**(1/2) ## radius of influence for ith analysis point\n",
    "        index_roi = np.where(radius <= ROI_arr[0])[0] ## only these obs will affect analysis value\n",
    "        \n",
    "        # calculate weights for each observation point using Cressman weight function\n",
    "        weights = np.zeros(len(x_ob)) ## create empty matrix for weights\n",
    "        for ä in index_roi:\n",
    "            w = cressman(radius[ä], ROI_arr[0]) # call the Cressman function created previously\n",
    "            weights[ä] = w # store the calculated values\n",
    "        \n",
    "        # these are the analysis values at the grid points (f_A (r_i))\n",
    "        f_A[m,n,0] = np.sum(weights * fo) / np.sum(weights) #first pass                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca0a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### SECOND ANALYSIS; PROXY BACKGROUND ###\n",
    "## use previous analysis values to create background values at obs locations & better the analysis @ grid points\n",
    "## fa = fa(prev. run) + sum(weights * (fo-fb)) / sum(weights)\n",
    "\n",
    "## get background values at obs locations from prev. analysis run --> (f_A (r_k))\n",
    "#redefine index values for our obs to our grid\n",
    "ind_x = (x_ob - xo)/delta_x\n",
    "ind_y = (y_ob - yo)/delta_y\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "fb = np.zeros((len(x_ob), len(ROI_arr)))\n",
    "for ß in range(len(ind_x)):\n",
    "    # if the obs location is in our grid, do bilinear interpolation\n",
    "    if (ind_x[ß] >= 0 and ind_x[ß] < 21) and (ind_y[ß] >= 0 and ind_y[ß] < 27):\n",
    "        P = bilinear(ind_x[ß], ind_y[ß], 0)\n",
    "        \n",
    "    # if the obs location is outside of our grid, do a 'reverse Cressman'\n",
    "    else:\n",
    "        rel_xa = x_ob[ß] - x_grid ## subtract your grid from your obs to get...\n",
    "        rel_ya = y_ob[ß] - y_grid ## ...relative distance for our map\n",
    "        radius_a = (rel_xa**2 + rel_ya**2)**(1/2) ## radius of influence for ith obs point\n",
    "        index_roi_a = np.where(radius_a <= ROI_arr[0]) ## only these analyses will affect obs value\n",
    "        \n",
    "        ## do a weight function again, but now for giving weights to analyses to get that 'background value'\n",
    "        weights_a = np.zeros(x_grid.shape)\n",
    "        for µ in index_roi_a[0]:\n",
    "            for s in index_roi_a[1]:\n",
    "                w_a = cressman(radius_a[µ,s], ROI_arr[0]) \n",
    "                weights_a[µ,s] = w_a\n",
    "        P = np.sum(weights_a * f_A[:,:,0]) / np.sum(weights_a)\n",
    "    \n",
    "    ## these are the analysis values at the obs locations (f_A (r_k))\n",
    "    fb[ß,0] = P\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "## get new analysis values at grid points --> (f_A² (r_i))\n",
    "for m in range(len(x_grid)):\n",
    "    for n in range(len(x_grid[0])):\n",
    "        rel_x = x_grid[m,n] - x_ob # subtract your obs from your analysis to get...\n",
    "        rel_y = y_grid[m,n] - y_ob # ...relative distance for our map\n",
    "        radius = (rel_x**2 + rel_y**2)**(1/2) # radius of influence for ith analysis point\n",
    "        index_roi = np.where(radius <= ROI_arr[1])[0] # only these obs will affect analysis value\n",
    "        \n",
    "        # get weights for each ob to influence grid points\n",
    "        weights = np.zeros(len(x_ob))    \n",
    "        for ä in index_roi:\n",
    "            w = cressman(radius[ä], ROI_arr[1]) \n",
    "            weights[ä] = w\n",
    "        \n",
    "        # these are the analysis values at the grid points (f_A² (r_i))\n",
    "        f_A[m,n,1] = f_A[m,n,0] + (np.nansum(weights * (fo-fb[:,0])) / np.nansum(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418308e5-888b-41f7-9c67-98553d8c8427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### THIRD ANALYSIS; PROXY BACKGROUND ###\n",
    "## use previous analysis values to create better background values at obs locations & analysis @ grid points\n",
    "## fa = fa(prev. run) + sum(weights * (fo-fb)) / sum(weights)\n",
    "\n",
    "## get background values at obs locations from prev. analysis run --> (f_A² (r_k))\n",
    "for ß in range(len(ind_x)):\n",
    "    # if the obs location is in our grid, do bilinear interpolation\n",
    "    if (ind_x[ß] >= 0 and ind_x[ß] < 21) and (ind_y[ß] >= 0 and ind_y[ß] < 27):\n",
    "        P = bilinear(ind_x[ß], ind_y[ß], 1)\n",
    "    \n",
    "    # if the obs location is outside of our grid, do a 'reverse Cressman'\n",
    "    else:\n",
    "        rel_xa = x_ob[ß] - x_grid ## subtract your grid from your obs to get...\n",
    "        rel_ya = y_ob[ß] - y_grid ## ...relative distance for our map\n",
    "        radius_a = (rel_xa**2 + rel_ya**2)**(1/2) ## radius of influence for ith obs point\n",
    "        index_roi_a = np.where(radius_a <= ROI_arr[1]) ## only these analyses will affect obs value\n",
    "        \n",
    "        ## do a weight function again, but now for giving weights to analyses to get that 'background value'\n",
    "        weights_a = np.zeros(x_grid.shape)\n",
    "        for µ in index_roi_a[0]:\n",
    "            for s in index_roi_a[1]:\n",
    "                w_a = cressman(radius_a[µ,s], ROI_arr[1]) \n",
    "                weights_a[µ,s] = w_a\n",
    "        P = np.sum(weights_a * f_A[:,:,1]) / np.sum(weights_a)\n",
    "    \n",
    "    # these are the analysis values at the obs locations (f_A² (r_k))\n",
    "    fb[ß,1] = P\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "## get new analysis values at grid points --> (f_A³ (r_i))\n",
    "for m in range(len(x_grid)):\n",
    "    for n in range(len(x_grid[0])):\n",
    "        rel_x = x_grid[m,n] - x_ob # subtract your obs from your analysis to get...\n",
    "        rel_y = y_grid[m,n] - y_ob # ...relative distance for our map\n",
    "        radius = (rel_x**2 + rel_y**2)**(1/2) # radius of influence for ith analysis point\n",
    "        index_roi = np.where(radius <= ROI_arr[2])[0] # only these obs will affect analysis value\n",
    "        \n",
    "        # get weights for each ob to influence grid points\n",
    "        weights = np.zeros(len(x_ob))    \n",
    "        for ä in index_roi:\n",
    "            w = cressman(radius[ä], ROI_arr[2]) \n",
    "            weights[ä] = w\n",
    "        \n",
    "        # these are the analysis values at the grid points (f_A³ (r_i))\n",
    "        f_A[m,n,2] = f_A[m,n,1] + (np.nansum(weights * (fo-fb[:,1])) / np.nansum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de65410-3e7b-4424-8f43-1de524a5d033",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ANALYSES ARE COMPLETE SO NOW NEED TO SEE HOW THEY ALL COMPARE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALYSIS DIFFERENCES ###\n",
    "\n",
    "ad = np.zeros((x_n, y_n, len(ROI_arr)))\n",
    "\n",
    "## second pass - first pass\n",
    "ad[:,:,0] = f_A[:,:,1] - f_A[:,:,0]\n",
    "\n",
    "## third pass - first pass\n",
    "ad[:,:,1] = f_A[:,:,2] - f_A[:,:,0]\n",
    "\n",
    "## third pass - second pass\n",
    "ad[:,:,2] = f_A[:,:,2] - f_A[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285a941-c9ee-4bdc-ad11-dbc8d242346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROOT MEAN SQUARE DIFFERENCES ###\n",
    "# calculate Root Mean Square Differences between analyses & obs for each pass\n",
    "# rms = sqrt (sum(fo-fb)²/K)\n",
    "\n",
    "rms = np.zeros((len(ROI_arr)))\n",
    "\n",
    "## first pass\n",
    "fp_dif = (fo-fb[:,0])**2 #fb is our f_A (r_k) values\n",
    "fp_dif = fp_dif[~np.isnan(fp_dif)] #delete the nan values\n",
    "rms[0]=np.sqrt((np.sum(fp_dif))/len(fp_dif))\n",
    "\n",
    "\n",
    "## second pass\n",
    "sp_dif = (fo-fb[:,1])**2 #fb is our f_A² (r_k) values\n",
    "sp_dif = sp_dif[~np.isnan(sp_dif)]  #delete the nan values\n",
    "rms[1]=np.sqrt((np.sum(sp_dif))/len(sp_dif))\n",
    "\n",
    "\n",
    "## third pass\n",
    "# need to calculate (f_A³ (r_k)) for our fb values\n",
    "for ß in range(len(ind_x)):\n",
    "    # if the obs location is in our grid, do bilinear interpolation\n",
    "    if (ind_x[ß] >= 0 and ind_x[ß] < 21) and (ind_y[ß] >= 0 and ind_y[ß] < 27):\n",
    "        P = bilinear(ind_x[ß], ind_y[ß], 2)\n",
    "    \n",
    "    # if the obs location is outside of our grid, do a 'reverse Cressman'\n",
    "    else:\n",
    "        rel_xa = x_ob[ß] - x_grid ## subtract your grid from your obs to get...\n",
    "        rel_ya = y_ob[ß] - y_grid ## ...relative distance for our map\n",
    "        radius_a = (rel_xa**2 + rel_ya**2)**(1/2) ## radius of influence for ith obs point\n",
    "        index_roi_a = np.where(radius_a <= ROI_arr[2]) ## only these analyses will affect obs value\n",
    "        \n",
    "        ## do a weight function again, but now for giving weights to analyses to get that 'background value'\n",
    "        weights_a = np.zeros(x_grid.shape)\n",
    "        for µ in index_roi_a[0]:\n",
    "            for s in index_roi_a[1]:\n",
    "                w_a = cressman(radius_a[µ,s], ROI_arr[2]) \n",
    "                weights_a[µ,s] = w_a\n",
    "        P = np.sum(weights_a * f_A[:,:,2]) / np.sum(weights_a)\n",
    "    \n",
    "    #these are the analysis values at the obs locations (f_A³ (r_k))\n",
    "    fb[ß,2] = P\n",
    "###############################################################################################################\n",
    "tp_dif = (fo-fb[:,2])**2 #fb is our (f_A³ (r_k)) values (from loop above)\n",
    "tp_dif = tp_dif[~np.isnan(tp_dif)]  #delete the nan values\n",
    "rms[2]=np.sqrt((np.nansum(tp_dif))/len(tp_dif))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfda40-4ec5-4e88-a78d-6a4b47dac512",
   "metadata": {},
   "source": [
    "### PLOT ANALYSES & ANALYSIS DIFFERENCES ON A MAP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8ecdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### PLOT 500mb ANALYSES ON A MAP ###\n",
    "\n",
    "## first pass (f_A (r_i))\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,f_A[:,:,0],colors='firebrick',levels=np.arange(0,8000,60),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(0,8000,60))\n",
    "plt.title(\"First Pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\First_Pass.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## second pass (f_A² (r_i))\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,f_A[:,:,1],colors='olivedrab',levels=np.arange(0,8000,60),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(0,8000,60))\n",
    "plt.title(\"Second Pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Second_Pass.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## third pass (f_A³ (r_i))\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,f_A[:,:,2],colors='steelblue',levels=np.arange(0,8000,60),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(0,8000,60))\n",
    "plt.title(\"Third Pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Third_Pass.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70d076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### PLOT THE ANALYSIS DIFFERENCES ###\n",
    "\n",
    "## second pass - first pass\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,ad[:,:,0],colors='saddlebrown',levels=np.arange(-60,90,20),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(-60,90,20))\n",
    "plt.title(\"second pass - first pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Second-First.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## third pass - second pass\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,ad[:,:,2],colors='lightseagreen',levels=np.arange(-60,60,15),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(-60,60,15))\n",
    "plt.title(\"third pass - second pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Third-Second.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## third pass - first pass\n",
    "proj = ccrs.Stereographic(central_longitude=-115,central_latitude=90,true_scale_latitude=60)\n",
    "fig = plt.figure(figsize=(8,8),dpi=200)\n",
    "ax1 = fig.add_subplot(111,projection=proj)\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.add_feature(cfeature.COASTLINE)\n",
    "## plot grid lat & lons because we want analysis locations NOT obs locations\n",
    "cs1 = ax1.contour(grid_lon,grid_lat,ad[:,:,1],colors='purple',levels=np.arange(-150,100,25),transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs1,levels=np.arange(-150,100,25))\n",
    "plt.title(\"third pass - first pass\")\n",
    "#plt.savefig(\"C:\\\\Users\\\\julia\\\\Documents\\\\GitHub\\\\ATSC528_2022\\\\02-Successive_Corrections\\\\Third-First.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ddfd2-6f03-483f-af58-9cc0a892fe3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STORE EVERYTHING IN TEXT FILES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store the analyses in text files ###\n",
    "with open('analysis_values.txt', 'wb') as f:\n",
    "    for line in f_A:\n",
    "        np.savetxt(f, line)\n",
    "\n",
    "\n",
    "### Store the 'background values' in text files ###\n",
    "fb_file = open('background_values.txt', 'w+')\n",
    "fb_string = str(fb)\n",
    "fb_file.write(fb_string)\n",
    "fb_file.close()\n",
    "        \n",
    "        \n",
    "### Store the difference fields in text files ###\n",
    "with open('difference_fields.txt', 'wb') as f:\n",
    "    for line in ad:\n",
    "        np.savetxt(f, line)\n",
    "        \n",
    "        \n",
    "### Store RMS values in text file ###\n",
    "rms_file = open('root_mean_square_differences.txt', 'w+')\n",
    "rms_string = str(rms)\n",
    "rms_file.write(rms_string)\n",
    "rms_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8eb7e-e5b6-4a24-b1e6-4eb0c21d67b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ~fin code ###"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd5cea41-ba84-402e-afd6-029b705ab35f",
   "metadata": {},
   "source": [
    "In a seperate text file (or below), answer the following questions\n",
    "'''\n",
    "1 - Describe the general features that you see in your contoured analyses.\n",
    "    \n",
    "    >> First Pass: There is a large negatively-tilted, trough axis over the Rocky Mountains stretching from the Candian border (@ ID/MT) to the Mexican \n",
    "       border (@AZ/NM). The 5400m line is relatively far north (north of Lake Winnipeg & through Red Deer, AB), which implies a 'not-that-cold' time of \n",
    "       year, amybe spring or fall. This is confirmed by looking at the date on the data set which says March, 13, 2019. There is a ridge over the eastern \n",
    "       third of the country, stretching from Florida to Ontario. Applying this week's Lab 110 topics, this looks like merdional flow.\n",
    "      \n",
    "    >> Second Pass: There is a similar trough as the first pass, but it is more negatively-tilted and deeper. The 5460m contour went from southern ID to \n",
    "       northern NM. Overall these contours dip far, so the trough is pretty deep. There is a slight ridge, but it's not as pronounced. The 5400m line is in\n",
    "       the northern mountains of MT, which is the southernmost extent, which still implies a spring- or falltime season. There is a very large gap between \n",
    "       the 5400m and the 5460m contour in the West. \n",
    "       \n",
    "    >> Third Pass: The trough is very negatively tilted and deep. The 5460m line is now in Amarillo, TX when in the second pass map it was in northern NM.\n",
    "       There is a strong 'height gradient' in Texas implying strong winds at 500mb. The ridge is still slightly visible over the eastern US. The 5400m contour\n",
    "       is now in Ketchum, ID at its southernmost extent, but goes back to north of Lake Winnipeg. Still springtime, but possibly colder than average temps in \n",
    "       the mountains (spring break skiing!). This could be the beginning of wacky analysis, as over Québec there is some weird ridge-like activity that looks\n",
    "       artificial (hints that we are approaching too many passes).\n",
    "\n",
    "2 - Describe the differences that you see in your contoured analyses.  \n",
    "    Does one analysis seem to be smoother than the other?  If so, what would cause this?\n",
    "    \n",
    "    >> Second Pass - First Pass: The ridge in the southeast got slightly higher as seen in the analysis difference plots. The contours there range from 0 in \n",
    "       New England and increase to +40 in Mississippi. The greatest positive difference was in the Gulf of Mexico, but we have no obs close (Corpus Chrisi and\n",
    "       Key West look to be closest), so this could just be model funkiness. The trough changed more, as height contours in the NM/TX border region decreased by\n",
    "       ~60m. A large area of the western US seems to have decreased in height (ecept for California, which increased, but this could again just be a product\n",
    "       of sparse obs available). Interestingly, the heights in provincial Canada also decreased a lot (-40m to -60m), but this, again, could be due to a lack\n",
    "       of obs available that causes more dramatic changes.\n",
    "       \n",
    "    >> Third Pass - Second Pass: Most of the bigger differences in heights are in the southwest US where the trough is located. The highest contour is a -60\n",
    "       which suggests that in some locations the heights fell by 60m (a lot). To the east and west of this 'negative bullseye', there's sharp increases, but\n",
    "       this could just be a mirage of sparse obs that lead to extrapolation. Additionally, the ridge in the eastern US did not change much, as over its \n",
    "       location there is a 0m and +15m contour. Over the Hudson Bay and northern Québec the heights start to do some funky stuff as there are weird, unrealistic\n",
    "       differences (large height rises & falls next to each other). For example, Sakami, QC sees height rises of 45m while 100 miles away in the Bay west of \n",
    "       Nunavut Land Claims Agreement, there are height falls of 45m. This is unrealistic and is signs of the analysis method failing at its borders.\n",
    "       Overall, the third-second plot had the least range in its contours (-60 to +60), which is expected as the analyses should be getting better with each pass.\n",
    "       \n",
    "    >> Third Pass - First Pass: This one has the greatest differences in heights which is to be expected as it is an uncorrected analysis compared to our \n",
    "       greatest corrected analysis. The trough strengthens immensely (height falls of up to -125m around Amarillo, TX). The more southern parts of the ridge\n",
    "       saw the greatest height rises of +50m in LA/MS. The east coast generally saw somewhere between +25 and +50m height rises. The 0m contour runs through\n",
    "       the northern part of the ridge, so less change there. The gradient in Texas is very large as Amarillo/Lubbock are around -125/-100m and Houston is at \n",
    "       +75m. Northern provincial Canada is also seeing the greatest differences, but this could be due to domain edge/sparse obs. The Hudson Bay sees HUGE \n",
    "       pressure decreases (-150m), but also has an extrememly sharp gradient as Northern Ontario has a height falls of 'only' -50m. Additionally, the Pacific\n",
    "       Coast has large height rises values as well (+50m to +75m).\n",
    "       \n",
    "     The first pass was the smoothest which makes sense as the large-scale features come out first and then the small-scale features get 'analyzed'. The third\n",
    "     pass looks a tad more jagged, which makes sense as the small-scale features, such as shortwaves, would be revealed the more passes get done. Also, the \n",
    "     first pass used the largest RoI, so it makes sense that everything gets smoothed out, whereas the third pass used the smallest RoI.\n",
    "    \n",
    "\n",
    "3 - What happens as you increase the number of successive correction passes?  Is this desirable?  Why or why not?\n",
    "    \n",
    "    As the number of successive correction passes gets increased, small-scale features come to light more and cause the analysis to be less smooth. Also, at\n",
    "    some point (could be 10, could be 666 passes) the analysis values will converge to the observation values at the observation locations. This is only \n",
    "    desirable when the observation values are perfect (no gross error, instrumentation error, or representativeness error = practically impossible), as this\n",
    "    will ultimately give 0 weight to the background. Typically, since our obs are flawed, we want to leverage the obs and background together to get better\n",
    "    values. In order to avoid this issue of converging to obs, you just cut off the amount of passes that get done, which creates a sort of subjective \n",
    "    objective analysis. But it all depends on if you are looking for large-scale features for 110 Lab students to locate troughs & ridges, or if you'd like\n",
    "    to actually analyze shortwave patterns.\n",
    "    \n",
    "    Important! Start with a large RoI first and then go to smaller RoI's so that you can capture the low frequency features (large-scale) before the high \n",
    "    frequency features (small-scale) get added in.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
